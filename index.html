<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Network Failure Prediction</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Neural Network for Predicting Network Failure</h1>
        <p>Exploring AI-driven solutions to enhance network reliability</p>
    </header>

    <nav>
        <a href="#about">About</a>
        <a href="#proposal">Proposal</a>
        <a href="#update">Midterm Update</a>
        <a href="#update2">Update 3</a>
        <a href="#final">Final Report</a>
    </nav>

    <section id="about">
        <h2>About the Project</h2>
        <p>Our research focuses on leveraging neural networks to predict network failures. By analyzing traffic patterns and system metrics, we aim to enhance the reliability of network infrastructures and prevent potential downtimes.</p>
    </section>

    <section id="proposal">
        <h2>Project Proposal</h2>
        <p><strong>Title:</strong> Neural Network for Predicting Network Failure</p>
        <p><strong>Objective:</strong> To develop a predictive model using neural networks that can analyze network traffic and system metrics to identify potential failure points before they occur.</p>
        <p><strong>Approach:</strong> Our approach includes traffic pattern analysis, system metric analysis, failure simulations using ns-3 or Mininet, and hybrid modeling techniques. We will implement the solution in Python using TensorFlow or PyTorch.</p>
        <p><strong>Expected Outcome:</strong> A prototype system capable of accurately predicting network failures, contributing to proactive maintenance and improved network reliability.</p>
    </section>

    <!-- New Section for Midterm Update -->
    <section id="update">
        <h2>Midterm Update</h2>
        <p><strong>Dataset Selection:</strong> We have switched from our initial plan of using generic public datasets to adopting the 
            <em>SOFI dataset (‚ÄúSymptom-fault relationship for IP-network‚Äù)</em>. This dataset provides a labeled collection 
            of normal (‚ÄúNE‚Äù) versus faulty (‚ÄúF‚Äù) network states captured from a large, emulated IP network. It includes 
            SNMP-based performance metrics and covers a range of artificially induced failures such as link down events, 
            line card failures, and high link utilization.</p>
        <p><strong>Reason for the Change:</strong> The SOFI dataset‚Äôs clear labeling and rich feature set (e.g., inbound/outbound 
            packets, error rates, operational status) make it ideal for training our predictive model. It also aligns 
            with real-world conditions where failure instances can be rare.</p>
        <p><strong>Progress:</strong> 
            <ul>
                <li>Preprocessed the dataset (label encoding, scaling) and confirmed class imbalance (~1.5% failures). 
                    We are addressing this through class weighting and potential oversampling methods.</li>
                <li>Developed an initial feedforward neural network, achieving high accuracy but a moderate recall 
                    for the minority ‚Äúfailure‚Äù class.</li>
                <li>Planned next steps include comparing other ML algorithms (Random Forest, Logistic Regression) 
                    and incorporating time-based splitting to capture sequential patterns more effectively.</li>
            </ul>
        </p>
    </section>

    <section id="update2">
        <h2>Biweekly Update #3</h2>
        <p><strong>Progress Overview:</strong><br>
        Over the past two weeks, we have made substantial progress in building and evaluating machine learning models for network failure prediction using the <em>SOFI (Symptom-Fault relationship for IP-Network)</em> dataset. This dataset, developed to capture symptom-fault causal relationships in IP-based enterprise networks, provided a rich and well-labeled foundation for our experiments.</p>

        <p>Our initial focus was on implementing two baseline models using <strong>PyTorch</strong> in <strong>Google Colab</strong>:</p>

        <ul>
            <li><strong>Supervised Learning Model:</strong> Built using a basic feedforward neural network. It was trained on labeled instances of the SOFI dataset, using cross-entropy loss and accuracy as evaluation metrics. This model performed well overall and showed promise in detecting patterns indicative of faults.</li>
            <li><strong>Unsupervised Learning Model:</strong> Constructed as an autoencoder, this model learned to reconstruct input data from normal (healthy) instances. High reconstruction error was used as an indicator of anomaly, allowing the model to identify unusual network behaviors without relying on labels.</li>
        </ul>

        <p><strong>Updated Project Direction:</strong><br>
        Based on our results, we‚Äôve expanded the scope of our project to compare four different machine learning approaches:</p>

        <ol>
            <li><strong>Supervised Learning</strong> (Completed)</li>
            <li><strong>Unsupervised Learning</strong> (Completed)</li>
            <li><strong>Semi-Supervised Learning</strong> (In Progress) ‚Äì Combines labeled and unlabeled data to improve generalization while reducing reliance on fully labeled datasets. We plan to experiment with pseudo-labeling and consistency regularization techniques.</li>
            <li><strong>Time-Dependent (Temporal) Model</strong> (Upcoming) ‚Äì Will involve sequential models such as RNNs or LSTMs to account for temporal dependencies in network behavior. This model aims to capture how patterns evolve over time, which is essential for forecasting failures.</li>
        </ol>

        <p><strong>Next Steps:</strong></p>
        <ul>
            <li>Complete semi-supervised model and evaluate performance.</li>
            <li>Preprocess SOFI data into time-series format for temporal model development.</li>
            <li>Compare all models using performance metrics such as accuracy, precision/recall, and false positive rate.</li>
            <li>Begin drafting our final report and preparing visualization material.</li>
        </ul>

        <p><strong>Challenges:</strong><br>
        Our primary challenges involve preparing the data in a format compatible with each model type and ensuring fair comparisons between them. Time-series preprocessing, in particular, is a focus for the coming week. Additionally, handling the dataset‚Äôs class imbalance across model types continues to be an area we‚Äôre actively addressing.</p>

        <p>Overall, we‚Äôre excited by our progress and believe the comparative evaluation of these models will provide valuable insights for real-world network failure prediction applications.</p>
    </section>

   
    <section id="final">
    <h2>Final Report Summary</h2>

    <p><strong>Dataset:</strong> We used the SOFI CoreSwitch-II dataset, which captures over 12,000 labeled network states with performance metrics and artificially induced failures. The dataset is highly imbalanced, with failures comprising less than 2% of records.</p>

    <p><strong>Preprocessing:</strong> We cleaned the data by removing placeholder values, dropped low-variance features, engineered error-based ratios, and scaled inputs. To address class imbalance, we used class weighting and SMOTE for supervised models.</p>

    <p><strong>Models Implemented:</strong></p>
    <ul>
        <li><strong>Random Forest:</strong> Served as our baseline. Achieved 68% recall and 30% precision on failures using threshold tuning. Performed best out of standalone models.</li>
        <li><strong>Autoencoder:</strong> Unsupervised model trained on normal data. Detected 61% of failures using reconstruction error but had a lower precision of 22%.</li>
        <li><strong>Feedforward Neural Network:</strong> Supervised model trained using binary cross-entropy and dropout. Achieved 64% recall and 38% precision on failures.</li>
        <li><strong>Support Vector Machine:</strong> Achieved 39% recall and 44% precision. Good at precision but less sensitive to failure cases.</li>
        <li><strong>Semi-Supervised Model:</strong> Combined autoencoder reconstruction error with Random Forest. Achieved our best results: 71% recall, 50% precision, and an F1-score of 0.59.</li>
        <li><strong>LSTM Model:</strong> A sequence model that underperformed due to weak temporal structure in the dataset. Only detected 4% of failures.</li>
    </ul>

    <p><strong>Key Takeaways:</strong></p>
    <ul>
        <li>Class imbalance must be addressed to get meaningful results.</li>
        <li>Simpler models (e.g., Random Forest) performed surprisingly well.</li>
        <li>Combining models provided the best balance between recall and precision.</li>
        <li>LSTM was not suitable without clearer time-based patterns.</li>
        <li>Preprocessing and threshold tuning were more impactful than model complexity in many cases.</li>
    </ul>

    <p><strong>Conclusion:</strong> This project gave us hands-on experience with a wide range of modeling strategies for anomaly detection. Our findings reinforce that model selection, data handling, and evaluation strategy all contribute significantly to real-world performance.</p>
    <p>
    <a href="final_report.pdf" target="_blank">üìÑ View Full Final Report (PDF)</a>
    </p>   
    </section>

     <footer>
        <p>&copy; 2025 Neural Network for Predicting Network Failure. All rights reserved.</p>
    </footer>
</body>
</html>

